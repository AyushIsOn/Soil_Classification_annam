{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11947555,"sourceType":"datasetVersion","datasetId":7511092}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, applications\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, classification_report, confusion_matrix\nfrom sklearn.utils.class_weight import compute_class_weight\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\ndef analyze_competition_structure():\n    \"\"\"Analyze the competition structure to understand what we need to submit\"\"\"\n    print(\"ANALYZING COMPETITION STRUCTURE\")\n    print(\"=\" * 60)\n    \n    base_dir = Path(\"/kaggle/input/soil-classification/soil_competition-2025\")\n    test_dir = base_dir / \"test\"\n    sample_submission_path = base_dir / \"sample_submission.csv\"\n    test_ids_path = base_dir / \"test_ids.csv\"\n    \n    # Check sample submission\n    print(\"Sample submission analysis:\")\n    if sample_submission_path.exists():\n        sample_df = pd.read_csv(sample_submission_path)\n        print(f\"  Sample submission entries: {len(sample_df)}\")\n        print(f\"  Columns: {list(sample_df.columns)}\")\n        print(f\"  First few entries:\")\n        print(sample_df.head())\n    \n    # Check test_ids.csv if it exists\n    print(\"\\nTest IDs file analysis:\")\n    if test_ids_path.exists():\n        test_ids_df = pd.read_csv(test_ids_path)\n        print(f\"  Test IDs file entries: {len(test_ids_df)}\")\n        print(f\"  Columns: {list(test_ids_df.columns)}\")\n        print(f\"  First few entries:\")\n        print(test_ids_df.head())\n    else:\n        print(\"  No test_ids.csv file found\")\n    \n    # Check actual test directory\n    print(f\"\\nTest directory analysis:\")\n    if test_dir.exists():\n        test_files = [f.name for f in test_dir.glob(\"*\") if f.is_file()]\n        print(f\"  Actual test files found: {len(test_files)}\")\n        print(f\"  First 10 test files:\")\n        for i, file in enumerate(test_files[:10]):\n            print(f\"    {i+1}. {file}\")\n        \n        return test_files\n    else:\n        print(\"  Test directory not found\")\n        return []\n\n# Run analysis first\ntest_files = analyze_competition_structure()\n\nclass KaggleSoilClassificationPipeline:\n    def __init__(self, base_dir=\"/kaggle/input/soil-classification/soil_competition-2025\", \n                 img_size=(384, 384), batch_size=32):\n        \"\"\"\n        Initialize the soil classification pipeline for Kaggle environment\n        \"\"\"\n        self.base_dir = Path(base_dir)\n        self.img_size = img_size\n        self.batch_size = batch_size\n        \n        # Kaggle paths\n        self.train_dir = self.base_dir / \"train\"\n        self.test_dir = self.base_dir / \"test\"\n        self.train_labels_path = self.base_dir / \"train_labels.csv\"\n        self.sample_submission_path = self.base_dir / \"sample_submission.csv\"\n        self.test_ids_path = self.base_dir / \"test_ids.csv\"\n        \n        # Output directory for Kaggle\n        self.output_dir = Path(\"/kaggle/working\")\n        \n        # Model storage\n        self.models = {}\n        self.class_weights = None\n        \n        # Setup GPU for Kaggle\n        self.setup_kaggle_gpu()\n        \n    def setup_kaggle_gpu(self):\n        \"\"\"Setup GPU configuration for Kaggle\"\"\"\n        print(\"\\nSetting up Kaggle GPU configuration...\")\n        \n        # Check GPU availability\n        gpus = tf.config.experimental.list_physical_devices('GPU')\n        if gpus:\n            try:\n                # Enable memory growth\n                for gpu in gpus:\n                    tf.config.experimental.set_memory_growth(gpu, True)\n                \n                # Enable mixed precision for faster training on Kaggle\n                policy = tf.keras.mixed_precision.Policy('mixed_float16')\n                tf.keras.mixed_precision.set_global_policy(policy)\n                \n                print(f\"Kaggle GPU setup complete. Found {len(gpus)} GPU(s)\")\n                print(\"Mixed precision enabled for faster training\")\n                \n            except RuntimeError as e:\n                print(f\"GPU setup error: {e}\")\n        else:\n            print(\"No GPU found, using CPU (will be much slower)\")\n            \n    def load_and_analyze_data(self):\n        \"\"\"Load training data and perform analysis\"\"\"\n        print(\"\\nLoading and analyzing dataset...\")\n        \n        # Load labels\n        train_df = pd.read_csv(self.train_labels_path)\n        print(f\"Loaded {len(train_df)} training labels\")\n        \n        # Convert labels to strings for ImageDataGenerator\n        train_df['label_str'] = train_df['label'].astype(str)\n        \n        # Create full paths and filter existing files\n        train_df['full_path'] = train_df['image_id'].apply(\n            lambda x: str(self.train_dir / x)\n        )\n        \n        # Check which files actually exist\n        existing_mask = train_df['full_path'].apply(os.path.exists)\n        train_df = train_df[existing_mask].reset_index(drop=True)\n        \n        print(f\"Dataset Statistics:\")\n        print(f\"  Total training images found: {len(train_df)}\")\n        \n        class_counts = train_df['label'].value_counts().sort_index()\n        for label, count in class_counts.items():\n            percentage = (count / len(train_df)) * 100\n            class_name = \"Non-Soil\" if label == 0 else \"Soil\"\n            print(f\"    Class {label} ({class_name}): {count} images ({percentage:.1f}%)\")\n        \n        # Check if we have both classes\n        unique_labels = train_df['label'].unique()\n        if len(unique_labels) == 1:\n            print(\"WARNING: Only one class found in training data!\")\n            self.handle_single_class_dataset(train_df)\n        \n        # Calculate class weights for imbalanced dataset\n        if len(unique_labels) > 1:\n            self.class_weights = compute_class_weight(\n                'balanced',\n                classes=np.unique(train_df['label']),\n                y=train_df['label']\n            )\n            self.class_weights = dict(zip(np.unique(train_df['label']), self.class_weights))\n        else:\n            self.class_weights = None\n        \n        return train_df\n    \n    def handle_single_class_dataset(self, train_df):\n        \"\"\"Handle the case where we only have one class in training data\"\"\"\n        print(\"\\nHandling single-class dataset...\")\n        \n        n_samples = len(train_df)\n        n_negative = min(200, n_samples // 4)  # 25% as negative examples\n        \n        # Randomly select some indices to be negative\n        negative_indices = np.random.choice(n_samples, n_negative, replace=False)\n        \n        # Create negative examples\n        train_df.loc[negative_indices, 'label'] = 0\n        train_df.loc[negative_indices, 'label_str'] = '0'\n        \n        print(f\"  Created {n_negative} artificial negative examples\")\n        \n        # Update class distribution\n        class_counts = train_df['label'].value_counts().sort_index()\n        for label, count in class_counts.items():\n            percentage = (count / len(train_df)) * 100\n            class_name = \"Non-Soil\" if label == 0 else \"Soil\"\n            print(f\"    Class {label} ({class_name}): {count} images ({percentage:.1f}%)\")\n    \n    def create_data_generators(self, train_df, validation_split=0.2):\n        \"\"\"Create optimized data generators with augmentation\"\"\"\n        print(f\"\\nCreating data generators...\")\n        \n        # Split data stratified by class\n        train_split, val_split = train_test_split(\n            train_df, \n            test_size=validation_split, \n            stratify=train_df['label'],\n            random_state=42\n        )\n        \n        print(f\"  Training samples: {len(train_split)}\")\n        print(f\"  Validation samples: {len(val_split)}\")\n        \n        # Enhanced training data generator\n        train_datagen = ImageDataGenerator(\n            rescale=1./255,\n            rotation_range=30,\n            width_shift_range=0.2,\n            height_shift_range=0.2,\n            shear_range=0.2,\n            zoom_range=0.2,\n            horizontal_flip=True,\n            vertical_flip=True,\n            brightness_range=[0.8, 1.2],\n            fill_mode='nearest'\n        )\n        \n        # Validation data generator\n        val_datagen = ImageDataGenerator(rescale=1./255)\n        \n        # Create generators using string labels\n        train_generator = train_datagen.flow_from_dataframe(\n            train_split,\n            x_col='full_path',\n            y_col='label_str',\n            target_size=self.img_size,\n            batch_size=self.batch_size,\n            class_mode='binary',\n            shuffle=True,\n            seed=42\n        )\n        \n        val_generator = val_datagen.flow_from_dataframe(\n            val_split,\n            x_col='full_path',\n            y_col='label_str',\n            target_size=self.img_size,\n            batch_size=self.batch_size,\n            class_mode='binary',\n            shuffle=False,\n            seed=42\n        )\n        \n        return train_generator, val_generator\n    \n    def create_efficientnet_model(self, version='B0'):\n        \"\"\"Create EfficientNet model with custom head\"\"\"\n        print(f\"\\nBuilding EfficientNet{version} model...\")\n        \n        if version == 'B0':\n            base_model = applications.EfficientNetB0(\n                weights='imagenet',\n                include_top=False,\n                input_shape=(*self.img_size, 3)\n            )\n        else:\n            raise ValueError(f\"Unsupported EfficientNet version: {version}\")\n        \n        # Freeze base model initially\n        base_model.trainable = False\n        \n        # Create model\n        model = models.Sequential([\n            base_model,\n            layers.GlobalAveragePooling2D(),\n            layers.BatchNormalization(),\n            layers.Dropout(0.4),\n            layers.Dense(256, activation='relu'),\n            layers.BatchNormalization(),\n            layers.Dropout(0.3),\n            layers.Dense(128, activation='relu'),\n            layers.BatchNormalization(),\n            layers.Dropout(0.2),\n            layers.Dense(1, activation='sigmoid', dtype='float32')\n        ])\n        \n        print(f\"EfficientNet{version} model created successfully\")\n        print(f\"  Total parameters: {model.count_params():,}\")\n        \n        return model\n    \n    def compile_model(self, model, learning_rate=0.001):\n        \"\"\"Compile model\"\"\"\n        optimizer = tf.keras.optimizers.AdamW(\n            learning_rate=learning_rate,\n            weight_decay=0.0001\n        )\n        \n        model.compile(\n            optimizer=optimizer,\n            loss='binary_crossentropy',\n            metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n        )\n        \n        return model\n    \n    def train_model(self, model, train_generator, val_generator, epochs=12):\n        \"\"\"Train model with early stopping\"\"\"\n        print(f\"\\nTraining model...\")\n        \n        callbacks = [\n            tf.keras.callbacks.EarlyStopping(\n                monitor='val_loss',\n                patience=4,\n                restore_best_weights=True,\n                verbose=1\n            ),\n            tf.keras.callbacks.ReduceLROnPlateau(\n                monitor='val_loss',\n                factor=0.5,\n                patience=2,\n                min_lr=1e-7,\n                verbose=1\n            )\n        ]\n        \n        history = model.fit(\n            train_generator,\n            epochs=epochs,\n            validation_data=val_generator,\n            callbacks=callbacks,\n            class_weight=self.class_weights,\n            verbose=1\n        )\n        \n        return history\n    \n    def evaluate_model(self, model, val_generator):\n        \"\"\"Evaluate model performance\"\"\"\n        print(\"\\nEvaluating model...\")\n        \n        # Get predictions\n        val_generator.reset()\n        predictions = model.predict(val_generator, verbose=1)\n        \n        # Test different thresholds for best F1 score\n        thresholds = np.arange(0.1, 0.9, 0.05)\n        f1_scores = []\n        \n        true_labels = val_generator.classes[:len(predictions)]\n        \n        for threshold in thresholds:\n            y_pred = (predictions > threshold).astype(int)\n            f1 = f1_score(true_labels, y_pred, zero_division=1)\n            f1_scores.append(f1)\n        \n        # Find best threshold\n        best_idx = np.argmax(f1_scores)\n        best_threshold = thresholds[best_idx]\n        best_f1 = f1_scores[best_idx]\n        \n        print(f\"  Best threshold: {best_threshold:.3f}\")\n        print(f\"  Best F1 score: {best_f1:.4f}\")\n        \n        # Final predictions with best threshold\n        predicted_classes = (predictions > best_threshold).astype(int).flatten()\n        accuracy = np.mean(predicted_classes == true_labels)\n        \n        print(f\"  Accuracy: {accuracy:.4f}\")\n        \n        return best_threshold, best_f1\n    \n    def create_complete_test_predictions(self, model, threshold=0.5):\n        \"\"\"Generate predictions for ALL test images\"\"\"\n        print(f\"\\nGenerating predictions for ALL test images...\")\n        \n        # Get ALL test files from the test directory\n        test_files = [f.name for f in self.test_dir.glob(\"*\") if f.is_file()]\n        test_files.sort()  # Sort for consistent ordering\n        \n        print(f\"Found {len(test_files)} test images\")\n        \n        if not test_files:\n            print(\"No test files found!\")\n            return pd.DataFrame()\n        \n        # Create dataframe for ALL test files\n        test_df = pd.DataFrame({'image_id': test_files})\n        test_df['full_path'] = test_df['image_id'].apply(lambda x: str(self.test_dir / x))\n        \n        print(f\"Processing {len(test_df)} test images...\")\n        \n        # Create test generator\n        test_datagen = ImageDataGenerator(rescale=1./255)\n        test_generator = test_datagen.flow_from_dataframe(\n            test_df,\n            x_col='full_path',\n            y_col=None,\n            target_size=self.img_size,\n            batch_size=self.batch_size,\n            class_mode=None,\n            shuffle=False  # Important: keep order for correct mapping\n        )\n        \n        # Generate predictions\n        print(\"Generating predictions...\")\n        predictions = model.predict(test_generator, verbose=1)\n        predicted_labels = (predictions > threshold).astype(int).flatten()\n        \n        # Create submission dataframe\n        submission_df = pd.DataFrame({\n            'image_id': test_files,\n            'label': predicted_labels\n        })\n        \n        print(f\"Generated predictions for {len(submission_df)} images\")\n        \n        # Display statistics\n        label_counts = submission_df['label'].value_counts().sort_index()\n        for label, count in label_counts.items():\n            percentage = (count / len(submission_df)) * 100\n            class_name = \"Non-Soil\" if label == 0 else \"Soil\"\n            print(f\"  Class {label} ({class_name}): {count} predictions ({percentage:.1f}%)\")\n        \n        return submission_df\n    \n    def save_submission(self, submission_df, filename):\n        \"\"\"Save submission file\"\"\"\n        submission_path = self.output_dir / filename\n        submission_df.to_csv(submission_path, index=False)\n        print(f\"Submission saved to: {submission_path}\")\n        print(f\"Total entries: {len(submission_df)}\")\n    \n    def run_pipeline(self):\n        \"\"\"Run the complete training pipeline\"\"\"\n        print(\"KAGGLE SOIL CLASSIFICATION PIPELINE\")\n        print(\"=\" * 60)\n        \n        # 1. Load and analyze data\n        train_df = self.load_and_analyze_data()\n        \n        # 2. Create data generators\n        train_gen, val_gen = self.create_data_generators(train_df)\n        \n        # 3. Create and train model\n        model = self.create_efficientnet_model('B0')\n        model = self.compile_model(model)\n        \n        # Train the model\n        history = self.train_model(model, train_gen, val_gen, epochs=12)\n        \n        # 4. Evaluate model\n        best_threshold, best_f1 = self.evaluate_model(model, val_gen)\n        \n        # 5. Generate predictions for ALL test images\n        submission = self.create_complete_test_predictions(model, threshold=best_threshold)\n        \n        # 6. Save submission with unique filename\n        import datetime\n        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        filename = f\"soil_classification_submission_{timestamp}.csv\"\n        \n        self.save_submission(submission, filename)\n        \n        # Also save as simple submission.csv for convenience\n        self.save_submission(submission, \"submission.csv\")\n        \n        print(f\"\\nPipeline complete!\")\n        print(f\"F1 Score: {best_f1:.4f}\")\n        print(f\"Optimal threshold: {best_threshold:.3f}\")\n        print(f\"Submission ready with {len(submission)} entries\")\n        print(f\"Files saved: {filename} and submission.csv\")\n        \n        return model, submission\n\ndef main():\n    \"\"\"Main function\"\"\"\n    pipeline = KaggleSoilClassificationPipeline()\n    model, submission = pipeline.run_pipeline()\n    \n    print(\"\\nSuccess! Check your submission files.\")\n    \n    # Display final submission preview\n    print(f\"\\nSubmission preview (showing first 10 and last 5 of {len(submission)} total):\")\n    print(submission.head(10))\n    print(\"...\")\n    print(submission.tail(5))\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:15:55.124968Z","iopub.execute_input":"2025-05-25T16:15:55.125269Z","iopub.status.idle":"2025-05-25T16:25:46.346282Z","shell.execute_reply.started":"2025-05-25T16:15:55.125245Z","shell.execute_reply":"2025-05-25T16:25:46.345556Z"}},"outputs":[{"name":"stdout","text":"ANALYZING COMPETITION STRUCTURE\n============================================================\nSample submission analysis:\n  Sample submission entries: 4\n  Columns: ['image_id', 'label']\n  First few entries:\n                               image_id  label\n0  6595f1266325552489c7d1635fafb88f.jpg      0\n1  4b614841803d5448b59e2c6ca74ea664.jpg      1\n2  ca30e008692a50638b43d944f46245c8.jpg      0\n3  e432d7988d125c8497d41b7ff223b187.jpg      1\n\nTest IDs file analysis:\n  Test IDs file entries: 967\n  Columns: ['image_id']\n  First few entries:\n                               image_id\n0  6595f1266325552489c7d1635fafb88f.jpg\n1  4b614841803d5448b59e2c6ca74ea664.jpg\n2  ca30e008692a50638b43d944f46245c8.jpg\n3  6a9046a219425f7599729be627df1c1a.jpg\n4  97c1e0276d2d5c2f88dddbc87357611e.jpg\n\nTest directory analysis:\n  Actual test files found: 967\n  First 10 test files:\n    1. 465084323936570da664f0ca8dc90326.jpg\n    2. 1aa0b12029d35e778dba5bff1255c638.jpg\n    3. 6df2c3dcd4fb59298c7a73467ea72eeb.jpg\n    4. 107f25ebd87f581ea57c630a2dcdf50c.jpg\n    5. dc35d58782615e4f9582c6b32c8b956e.jpg\n    6. c7af21ff925c51adb526c487148bac6d.jpg\n    7. e8bdb9805d455093ab4f9503cad8052b.jpg\n    8. 4e82cd9e403a5edeb92bee62410fd9b1.jpg\n    9. d7d5ebd1528852a0a0d5fc8ca175e196.jpg\n    10. 04af8a005d485bc291124bc288cc8d7f.jpg\n\nSetting up Kaggle GPU configuration...\nKaggle GPU setup complete. Found 2 GPU(s)\nMixed precision enabled for faster training\nKAGGLE SOIL CLASSIFICATION PIPELINE\n============================================================\n\nLoading and analyzing dataset...\nLoaded 1222 training labels\nDataset Statistics:\n  Total training images found: 1222\n    Class 1 (Soil): 1222 images (100.0%)\nWARNING: Only one class found in training data!\n\nHandling single-class dataset...\n  Created 200 artificial negative examples\n    Class 0 (Non-Soil): 200 images (16.4%)\n    Class 1 (Soil): 1022 images (83.6%)\n\nCreating data generators...\n  Training samples: 977\n  Validation samples: 245\nFound 972 validated image filenames belonging to 2 classes.\nFound 242 validated image filenames belonging to 2 classes.\n\nBuilding EfficientNetB0 model...\nEfficientNetB0 model created successfully\n  Total parameters: 4,417,188\n\nTraining model...\nEpoch 1/12\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 2s/step - accuracy: 0.5318 - auc: 0.4705 - loss: 0.8754 - val_accuracy: 0.8388 - val_auc: 0.4306 - val_loss: 0.5556 - learning_rate: 0.0010\nEpoch 2/12\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.6264 - auc: 0.5093 - loss: 0.6657 - val_accuracy: 0.8388 - val_auc: 0.4744 - val_loss: 0.5619 - learning_rate: 0.0010\nEpoch 3/12\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - accuracy: 0.7333 - auc: 0.4548 - loss: 0.6014 - val_accuracy: 0.8388 - val_auc: 0.4984 - val_loss: 0.4603 - learning_rate: 0.0010\nEpoch 4/12\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - accuracy: 0.8088 - auc: 0.4616 - loss: 0.5434 - val_accuracy: 0.8388 - val_auc: 0.5904 - val_loss: 0.4440 - learning_rate: 0.0010\nEpoch 5/12\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - accuracy: 0.7860 - auc: 0.4307 - loss: 0.5519 - val_accuracy: 0.8388 - val_auc: 0.4900 - val_loss: 0.4453 - learning_rate: 0.0010\nEpoch 6/12\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.8268 - auc: 0.5050 - loss: 0.4796 - val_accuracy: 0.8388 - val_auc: 0.5000 - val_loss: 0.4434 - learning_rate: 0.0010\nEpoch 7/12\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - accuracy: 0.8015 - auc: 0.5044 - loss: 0.5194 - val_accuracy: 0.8388 - val_auc: 0.4617 - val_loss: 0.4487 - learning_rate: 0.0010\nEpoch 8/12\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8198 - auc: 0.5002 - loss: 0.5093\nEpoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - accuracy: 0.8199 - auc: 0.4998 - loss: 0.5091 - val_accuracy: 0.8388 - val_auc: 0.4952 - val_loss: 0.4498 - learning_rate: 0.0010\nEpoch 9/12\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - accuracy: 0.8244 - auc: 0.4936 - loss: 0.4883 - val_accuracy: 0.8388 - val_auc: 0.5194 - val_loss: 0.4455 - learning_rate: 5.0000e-04\nEpoch 10/12\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - accuracy: 0.8427 - auc: 0.5294 - loss: 0.4582 - val_accuracy: 0.8388 - val_auc: 0.5286 - val_loss: 0.4432 - learning_rate: 5.0000e-04\nEpoch 11/12\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - accuracy: 0.8252 - auc: 0.5178 - loss: 0.4756 - val_accuracy: 0.8388 - val_auc: 0.5183 - val_loss: 0.4424 - learning_rate: 5.0000e-04\nEpoch 12/12\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.8366 - auc: 0.5399 - loss: 0.4537 - val_accuracy: 0.8388 - val_auc: 0.5023 - val_loss: 0.4424 - learning_rate: 5.0000e-04\nRestoring model weights from the end of the best epoch: 12.\n\nEvaluating model...\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 1s/step \n  Best threshold: 0.100\n  Best F1 score: 0.9124\n  Accuracy: 0.8388\n\nGenerating predictions for ALL test images...\nFound 967 test images\nProcessing 967 test images...\nFound 967 validated image filenames.\nGenerating predictions...\n\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 427ms/step\nGenerated predictions for 967 images\n  Class 1 (Soil): 967 predictions (100.0%)\nSubmission saved to: /kaggle/working/soil_classification_submission_20250525_162546.csv\nTotal entries: 967\nSubmission saved to: /kaggle/working/submission.csv\nTotal entries: 967\n\nPipeline complete!\nF1 Score: 0.9124\nOptimal threshold: 0.100\nSubmission ready with 967 entries\nFiles saved: soil_classification_submission_20250525_162546.csv and submission.csv\n\nSuccess! Check your submission files.\n\nSubmission preview (showing first 10 and last 5 of 967 total):\n                               image_id  label\n0  0057140e782b5b5aa01fc44cc9a871ed.jpg      1\n1  01d8f285ca985154a0fa5e824863ab27.jpg      1\n2  022fa6c0e0af5aa8af770e1a5927f993.jpg      1\n3  02cef51891f25c6fa33b6af311d533e1.jpg      1\n4  03fbf14891625344bdd6dc6b96c3014e.jpg      1\n5  0487c043d45956d79b3f770f7261e1a4.jpg      1\n6  04af8a005d485bc291124bc288cc8d7f.jpg      1\n7  04e5c019ff7851168e889f142dae0238.jpg      1\n8  057b207983775d56a535e51fb0c015b3.jpg      1\n9  05e31b9f45685fd7a00cbfb2b0a77a5b.jpg      1\n...\n                                 image_id  label\n962  fe532e68e15d5324805d1b9375762e9d.jpg      1\n963  fea145f2a3d254de9306fddcaa3004bb.jpg      1\n964  ff5ed999f18253028346072009c4c384.jpg      1\n965  ffbc1148e50f533f98cac1836f3e279b.jpg      1\n966  fff84c7761a05e2cae043855e995d520.jpg      1\n","output_type":"stream"}],"execution_count":5}]}